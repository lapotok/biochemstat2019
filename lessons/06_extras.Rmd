---
output: 
  html_document:
    code_folding: hide
editor_options: 
  chunk_output_type: console
---

```{r, echo=F}
knitr::opts_knit$set(root.dir = here::here())
suppressPackageStartupMessages(library(drc))
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(magrittr))
suppressPackageStartupMessages(library(cowplot))
suppressPackageStartupMessages(library(ggstance))

theme_set(theme_bw())
```

<!--
# Что осталось на конец

+ а у меня R не установлен... компьютер сломался и т.п.
  + online R console 
    (импорт данных: Excel copy-paste -> read.table(text="", sep="\t") )
  + online RStudio
+ отягчающие факторы при статистическом анализе
  + малая выборка
  + зависимость между наблюдениями/измерениями
    + неизвестные группы, неизвестная представленность
    + пространственная корреляция
    + временная корреляция (автокорреляция)
  + ненормальность распределения (residuals!)
  + гетероскедантичность и как с ней бороться
    + тесты с поправками
    + трансформация данных (?)
    + GLM
    + Contigency tables vs GLMs
  + ошибки измерения, типы реплик (технические и биологические)
  + выбросы
    + visual
    + метод median +- 1.5 IQR
    + outliers: grubbs.test(), dixon.test(), chisq.out.test() 
    + car::outlierTest
    + rosnerTest() in EnvStats
    + lofactor() in DMwR
    + dr4pl::OutlierDetection
    + cook distance / pareto k diagnostics 
    + residuals/standardized residuals
  + Множество гипотез, а не H0/H1
+ как объединять разные предсказания с разными ошибками - мета-анализ
+ регрессия как инструмент для нахождения зависимостей
  + подбор предикторов
    + мультиколлинеарность
    + качество фита vs переобучение
    + значимость коэффициентов - как интерпретировать
      + либо значимость есть, либо - не понятно; 
      + выкидывать ли? 
      + может быть полезен для предсказания!
    + вклад в объяснение дисперсии (adj R2/D2)
    + вклад в предсказание нового (CV, AIC)
    + деление на группы (эффект Симпсона)
    + ДАГи
      + вилка (общая причина)
      + коллайдер (ложная зависимость через общее влияние предиктора и ауткама)
      + цепи (блокировка сигнала по пути)
+ примеры из работы
  + Lowry/Bradford
  + ELISA/FIA
  + stability combine predictions
  + PAGE/WB
  + Octet baseline
  + фитирование нелинейных кривых (S-образных)
    + goodness of fit
    + сравнение углов наклона
    + вычитание базовой линии
  + границы линейной зависимости (концентрация, флуоресценция, кинетика)
  + сравнение чего-то по 3м точкам (форезы/блоты) - калибровки, ошибка и т.п.
  + много спектров
  + выявление влияния разных факторов на какой-то процесс
  + подбор оптимальных условий (ДОЕ)
+ предсказание и классификация (ML)
  + supervised vs unsupervised
    + discriminant analysis
    + Clustering
    + RandomForest
    + NNs
+ визуализация многомерных данных
  + PCA/tSNE/FA
  + dendrogram
  + heatmaps
+ визуализация кучи измерений
  + x, y (z)
  + color
  + alpha
  + size, linewidth
  + animation (time)
  + shape, linetype
  + facets
+ подходы к статистике
  + Frequentist
  + Bootstrap/Resampling
  + Bayesian
-->
# Значимость предикторов и выбор наилучшей модели [про что еще рассказать потом]

+ ANOVA, R2, G2
+ AIC, deviance, likelihood
+ prediction vs inference
  + fitting vs prediction accuracy, overfitting
  + multicollinearity
  + DAGs & fallacies
    + correlation vs causation (common cause)
    + non-independence (grouping: simpson's paradox)
    + blocking in information paths
    + common consequence (collider = inverse fork)

# Линейные модели и оптимизация условий (дизайн эксперимента, DoE)

Регрессия позволяет нам сделать модель, которая способна извлекать из данных информацию о характере завимисимости, а также потом делать предсказания для неизвестных значений данных. Это "умение" моделей можно использовать, например, при дизайне эксперимента. Если мы изучим, например, различные условия для наибольшей активности фермента (или для экспрессии белка), то собрав несколько экспериментальных точек с разными условиями мы сможем предсказать такое сочетание, которое бы дало наиболее высокий выход.

[Здесь](../methods/DoE_notes.html) описывается как это работает на примере максимизации выручки магазина, если мы меняем такие параметры, как цена на товар и высота расположения товара на полке.

# Нелинейная регрессия

Для некоторых случаев зависимость нельзя описать полиномом, либо зависимость описывается хорошо известным нелинейным уравнением. Для этого используется другая разновидность регрессии - нелинейная.

Типичными примерами, когда требуется использование нелинейной регрессии, являются

+ ферментативная кинетика (определение параметров уравнения Михаэлиса-Ментен);
+ измерение кинетики реакций для определения концентрации субстратов;
+ измерение кинетики связывания модекул для определения аффинности;
+ измерение концентрации.

Рассмотрим несколько примеров.

## Кинетика реакций: определение начальной скорости по зависимости концентрации от времени

Классическая задача - определение скорости реакции при разных условиях (например, при разной концентрации фермента). В данном примере речь идет об определении начальной скорости реакции образования 1,3-ФГК, каталицируемой ГАФД, по кривым увеличения оптической плотности при 340 нм в результате образования NADH (пример с практикума по энзимологии на 3 курсе).

> Активность ГАФД определяли спектрофотометрически при 340 нм по увеличению оптической плотности в результате образования NADH. Реакционная среда для определения активности ГАФД состоит из 0,1 М глицинового буфера (pH 8,9), 5 мМ ЭДТА, 1 мМ NAD+, 5 мМ Na3AsO4 и ГАФД (3 мл среды в кювете). Реакцию начинали внесением ГАФД. Вместо неорганического фосфата использовали арсенат, для того, чтобы сдвинуть реакцию в сторону образования продуктов, поскольку  1-арсенат-3-ФГК легко распадается на арсенат и 3-ФГК. Количество фермента подбирали экспериментально.

Откроем данные. В первой колонке - время измерения, сек. В остальных колонках - измерения (OD340) для разных количеств фермента, мкл.

```{r, fig.height=4, fig.width=5, cache=T}
# данные лежат в https://github.com/lapotok/biochemstat2019/tree/master/data
GAPDH_curves = rio::import("data/GAPDH_kinetic_curves.csv")

# Преобразуем файл в "длинный формат"
GAPDH_curves_long = 
  GAPDH_curves %>%
  pivot_longer(-time, "group") %>% 
  mutate(group = factor(group, levels=c(5,10,15)))

# Посмотрим на исходные данные
GAPDH_curves_long %>% 
  ggplot(aes(x=time, y=value, col=group)) +
  geom_point(alpha=.7, size=2) +
  labs(title="", x="Время, сек", y="OD340")
```

Теперь смоделируем каждую кривую и посмотрим на значения подобранных коэффициентов. Для моделирования этих кривых используем функцию `AR.3()`, обозначающую асимптотическую функцию (также в пакете `drc` имеется большое разнообразие других функций для других кривых - можно смотреть, какая функция подойдет лучше для конкретных нужд).

$$
f(t) = c + (d-c) \cdot \Big(1 - \exp\big(-t/e\big)\Big)
$$

```{r, cache=T}
m = drm(value ~ time, # формула
        group, # идентификаторы групп
        data = GAPDH_curves_long, # данные
        fct=AR.3()) # асимптотическая функция, которую используем для подгонки модели
m
```

Мы видим, что для каждой концентрации определились свои коэффициенты. Создадим предсказания с помощью построенной модели и изобразим их на графике.

```{r, fig.height=4, fig.width=5, cache=T}
# создаем таблицу с исходными данными по предсказаниям
pred_grid = expand.grid(group = levels(GAPDH_curves_long$group), time = seq(0, 60, length=100))
pred_grid$predicted = predict(m, newdata = pred_grid)
pred_grid %>% head()

# рисуем график
GAPDH_curves_long %>% 
  ggplot(aes(x=time, y=value, col=group)) +
  geom_point(alpha=.7, size=hatvalues(m)*30) +
  geom_line(aes(x=time, y=predicted), data=pred_grid, alpha=0.8, linetype="dotted") + 
  labs(x="Время, сек", y="OD340") + scale_color_discrete(name = "количество\nфермента")
```

Мы подобрали коэффициенты для построения модели всей кривой, для этого использовались все точки. При этом разные точки имеют разную важность для подбора коэффициентов. Точки большего размера (с краев диапазона) сильнее влияют на коэффициенты. Чем больше точек мы анализируем, тем больше информации получаем о форме кривой.

Теперь, если нам интересно оценить начальную скорость (касательную в нулевой точке) по полученной кривой, то мы можем использовать тот факт, что скорость - это производная концентрации по времени, $v = dC/dt$. К счастью, зная формулу для описания кинетической кривой мы можем легко получить в `R` и формулу для производной.

```{r, cache=T}
m_deriv = D(expression(c + (d-c) * (1 - exp(-t/e))), 't')
m_deriv
```

$$
\begin{aligned}
v(t) & = \frac{d\bigg(c + (d-c) \cdot \Big(1 - \exp\big(-t/e)\Big)\bigg)}{dt} = (d-c) \cdot \exp\big(-t/e)\cdot(1/e) \\
v(0) & = (d-c) \cdot \exp\big(-0/e)\cdot(1/e) = (d-c) \cdot (1/e) 
\end{aligned}
$$

Теперь по этой формуле мы можем вычислить начальную скорость для каждой кривой. А после этого можем построить прямую начальной скорости.

```{r, fig.height=4, fig.width=5, cache=T}
m_coeff = coef(m) # достаем коэффициенты
init_preds = tibble()
time_points = c(0, 60)
# считаем предсказания для начальной скорости
for (group in levels(GAPDH_curves_long$group)) {
  intercept = m_coeff[paste0("c:", group)]
  slope = (m_coeff[paste0("d:", group)] - m_coeff[paste0("c:", group)]) * (1/m_coeff[paste0("e:", group)])
  tmp = tibble(
    time = time_points,
    group = group,
    predicted =  slope * time_points
  )
  init_preds = bind_rows(init_preds, tmp)
}

GAPDH_curves_long %>% 
  ggplot(aes(x=time, y=value, col=group)) +
  geom_point(alpha=.7, size=hatvalues(m)*30) +
  geom_line(aes(x=time, y=predicted), data=pred_grid, alpha=0.8, linetype="dotted") + 
  geom_line(aes(x=time, y=predicted), data=init_preds, alpha=0.8) + 
  coord_cartesian(ylim=c(0, 0.15)) +
  labs(x="Время, сек", y="OD340") + scale_color_discrete(name = "количество\nфермента")
```

А теперь было бы интересно сравнить те кривые, которые получились при расчете с использованием всех точек кривых (пунктир) и те, которые получились при попытках визуально отобрать наиболее линейный участок (линии без пунктира).

```{r, fig.height=4, fig.width=5, cache=T, echo=F}
GAPDH_curves_long_linear1 = # visually
  GAPDH_curves %>%
  rownames_to_column() %>% 
  mutate(rowname = as.numeric(rowname)) %>% 
  pivot_longer(-(time:rowname), "group") %>% 
  mutate(group = factor(group, levels=c(5,10,15))) %>% 
  filter(!(group == "5" & (rowname < 2 | rowname > 5))) %>% 
  filter(!(group == "10" & (rowname < 2 | rowname > 6))) %>% 
  filter(!(group == "15" & (rowname < 2 | rowname > 4))) 
GAPDH_curves_long_linear_m1 = lm(value ~ time*group, GAPDH_curves_long_linear1)
GAPDH_curves_long_linear_predgrid1 = 
  expand.grid(group = levels(GAPDH_curves_long$group),
              time = seq(0, 60, l=200))
GAPDH_curves_long_linear_predgrid1$predicted = 
  predict(GAPDH_curves_long_linear_m1, newdata = GAPDH_curves_long_linear_predgrid1)
GAPDH_curves_long_linear2 = # visually
  GAPDH_curves %>%
  rownames_to_column() %>% 
  mutate(rowname = as.numeric(rowname)) %>% 
  pivot_longer(-(time:rowname), "group") %>% 
  mutate(group = factor(group, levels=c(5,10,15))) %>% 
  filter(!(group == "5" & (rowname < 2 | rowname > 4))) %>% 
  filter(!(group == "10" & (rowname < 2 | rowname > 5))) %>% 
  filter(!(group == "15" & (rowname < 2 | rowname > 3))) 
GAPDH_curves_long_linear_m2 = lm(value ~ time*group, GAPDH_curves_long_linear2)
GAPDH_curves_long_linear_predgrid2 = 
  expand.grid(group = levels(GAPDH_curves_long$group),
              time = seq(0, 60, l=200))
GAPDH_curves_long_linear_predgrid2$predicted = 
  predict(GAPDH_curves_long_linear_m2, newdata = GAPDH_curves_long_linear_predgrid2)

GAPDH_curves_long %>% 
  ggplot(aes(x=time, y=value, col=group)) +
  geom_point(size=.5, alpha=.7) +
  geom_point(size=2, alpha=.7, data = GAPDH_curves_long_linear1) +
  geom_line(aes(x=time, y=predicted, col=group), data = GAPDH_curves_long_linear_predgrid1, alpha=0.4) +
  geom_line(aes(x=time, y=predicted, col=group), data = GAPDH_curves_long_linear_predgrid2, alpha=0.4) +
  geom_line(aes(x=time, y=predicted), data=init_preds, alpha=0.8, linetype="dashed") + 
  coord_cartesian(ylim=c(0, 0.15)) +
  labs(x="Время, сек", y="OD340") + scale_color_discrete(name = "количество\nфермента")
```

Можно заметить, что на глаз мы стремимся проводить более горизонтальные прямые. Мы считаем, что для большей достоверности нам нужно больше точек, однако в данном случае это черевато недооценкой реальной скорости. Метод с производной позволяет извлекать из всех точек информацию о нормальной скорости.

## Пример анализа ферментативная кинетика пример

Уравнение Михаэлиса-Ментен используется для количественного описания ферментативной кинетики.

$$
v = \frac{V_{max} \cdot S}{K_M+S} = \frac{V_{max}}{\frac{K_M}{S}+1}
$$

Готовая функция `MM.2()` из пакета `drc` служит для моделирования кривых Михаэлис-Ментен. Она имеет следующие параметры:

$$
\begin{aligned}
& y = \frac{d}{{\large\frac{e}{x}+1}} \\
& y = v \\
& d = V_{max} \\
& e = K_M \\
& x = S
\end{aligned}
$$

В качесте примера рассмотрим задачу определения ферментативной кинетики с практикума по энзимологии на 3 курсе (определение каталитической активности ЛДГ по пирувату). Для каждой концентрации пирувата (s, мМ) имеется измерение скорости реакции (v, мкмоль/мин).

```{r, fig.height=3, fig.width=9, cache=T}
# данные доступны на https://github.com/lapotok/biochemstat2019/tree/master/data
d = rio::import("data/LDH_km_vmax.csv")

# преобразуем исходные данные в координаты 
# А) Лайнуивера-Берка, Б) Иди-Хофсти
d %<>% 
  mutate(is=1/s, iv=1/v, vs=v/s) 

# посмотрим на исходные данные в разных координатах
p_sv = 
  ggplot(d, aes(x=s,y=v)) + 
  geom_point(alpha=.6, size=2, col="dodgerblue") + 
  labs(subtitle = "Координаты Михаэлиса-Ментен")
p_isiv = 
  ggplot(d, aes(x=1/s,y=1/v)) + 
  geom_point(alpha=.6, size=2, col="dodgerblue") + 
  labs(subtitle = "Координаты Лайнуивера-Берка")
p_vvs = 
  ggplot(d, aes(x=v/s,y=v)) + 
  geom_point(alpha=.6, size=2, col="dodgerblue") + 
  labs(subtitle = "Координаты Иди-Хофсти")

plot_grid(p_sv, p_isiv, p_vvs, nrow = 1)
```

Можно заметить, что в разных координатах 

+ разброс точек ведет себя по-разному;
+ в случае Лайнуивера-Берка точки более скученны, а далеко отстоящие точки сильнее будут влиять на коэффициенты (неточно измеренные концентрации в точках с наименьшей концентрации будут особенно критичны);
+ в случае Иди-Хофсти обе координаты зависят от измеряемой величины - скорости, поэтому содержат ошибку измерения.

Таким образом, используемые методы линеаризации [искажают структуру ошибок](https://en.wikipedia.org/wiki/Michaelis%E2%80%93Menten_kinetics#Determination_of_constants) и приводят к нарушению предпосылок для использования линейных моделей.

На графиках ниже можно видеть кривые, подобранные разными методами. Разная величина точек отражает разный вклад (вес) этих точек в результат подбора коэффициентов кривой разными методами (для нелинейной регрессии вклад всех точек примерно одинаков, т.е. выше устойчивость к выбросам единичных точек). Для этих данных адекватные кривые получились с помощью метода нелинейной регрессии и линеаризации Иди-Хофсти (которых на других данных с большим вкладом ошибок может быть менее надежен).

```{r, fig.width=9, fig.height=3, cache=T}
m_mm = drm(v ~ s, data = d, fct = MM.2(names = c("Vmax", "Km"))) # red
m_lb = lm(iv ~ is, data = d) # blue
m_eh = lm(v ~ vs, data = d) # green

new_s = with(d, seq(min(s), max(s), l=200))
new_vs = with(d, seq(min(vs), max(vs), l=200))
predictions = data.frame(s=new_s, is=1/new_s, vs=new_vs)
predictions$mm = predict(m_mm, newdata = predictions)
predictions$lb = 1/predict(m_lb, newdata = predictions)
predictions$eh = predict(m_eh, newdata = predictions)
predictions$s_eh = predictions$eh/predictions$vs

predictions %>% 
  pivot_longer(-(s:vs)) -> predictions_long

# mm leverage
p_sv_mm = 
  ggplot(d, aes(x=s,y=v)) + 
  geom_point(aes(size=hatvalues(m_mm)), alpha=.6, col="dodgerblue") +
  geom_line(aes(x=s, y=mm), data=predictions, col="red", alpha=.5) +
  labs(subtitle = "AR.3():\nвеса точек и итоговая модель") + 
  theme(legend.position = "none")

p_sv_lb = 
  ggplot(d, aes(x=s,y=v)) + 
  geom_point(aes(size=hatvalues(m_lb)), alpha=.6, col="dodgerblue") +
  geom_line(aes(x=s, y=lb), data=predictions, col="orange", alpha=.5) +
  labs(subtitle = "Метод Лайнуивера-Берк:\nвеса точек и итоговая модель") + 
  theme(legend.position = "none")

p_sv_eh = 
  ggplot(d, aes(x=s,y=v)) + 
  geom_point(aes(size=hatvalues(m_eh)), alpha=.6, col="dodgerblue") +
  geom_line(aes(x=s_eh, y=eh), data=predictions, col="green", alpha=.5) +
  labs(subtitle = "Метод Иди-Хофсти:\nвеса точек и итоговая модель") + 
  theme(legend.position = "none")

plot_grid(p_sv_mm, p_sv_lb, p_sv_eh, nrow = 1)

m_mm
```



## Кинетика связывания [...]

Пример с октетом

+ удаление базовой линии
+ фитирование диссоциации
+ фитирование ассоциации

## Анализ кривых иммуноэссеев

Множество биологических процессов описывается S-образной кривой. Характерным примером является зависимость сигнала (оптическая плотность, флуоресценция, фосфоресценция и т.п.) от концентрации при анализе с помощью различных иммуноэссеев.

Мы рассмотрим пример построения нелинейной модели для S-образной калибровочной кривой. Зачастую исследователи находят линейный участок кривой и ограничиваются им, однако использование всех точек позволяет точнее установить форму зависимости и получить меньшие ошибки. Такой анализ будет нечувствителен к субъективному выбору точек для "линейного участка".

$$
signal = c + \frac{d-c}{\bigg(1 + \exp\Big(b\cdot\big(\log(conc)-\log(e) \big)\Big)\bigg)^f}
$$

В данной формуле все параметры имеют интерпретируемое значение: $b$ - наклон кривой ("крутизна"), $c$ - нижняя асимптота по оси (минимальный сигнал), $d$ - верхняя асимптота (максимальный сигнал), $e$ - сдвиг кривой по оси концентрации, $f$ - ассиметричность кривой относительно точки изгиба.

```{r cal_fit, cache=T, warning=F, message=F}
# данные с https://github.com/lapotok/biochemstat2019/tree/master/data
calibr = rio::import("data/igfbp4_3_cal_curve.csv")
# для моделирования используем 5PL функцию
m_drc = drm(signal ~ conc, data=calibr, fct=LL.5())
m_drc
# predictions
new_x = exp(seq(log(0.05), log(100), l=200))
pred = predict(m_drc, newdata = data.frame(conc=new_x), interval = "prediction")
```

После фитирования наложим предсказанные кривые на экспериментальные точки. Видно, что на всем диапазоне линия проходит близко к точкам, незначительные выбросы не оказывают влияние на форму кривой.

```{r cal_coords, cache=T, fig.width=8, fig.height=3, warning=F, message=F, echo=F}
pred %>% 
  as_tibble() %>% 
  mutate(conc = new_x) %>% 
  mutate(Lower = ifelse(Lower <= 0, 0.1, Lower)) -> pred
# inverse predictions
new_y = exp(seq(log(1e3), log(1e6), l=200))
invpred = ED(m_drc, respLev = new_y, type="absolute", interval = "delta", display = F)
invpred %>% 
  as_tibble() %>% 
  mutate(signal = new_y) -> invpred

invpred_polygon = 
  bind_rows(
    invpred %>% dplyr::select(signal, Upper) %>% rename(x=Upper, y=signal) %>% arrange(x),
    invpred %>% dplyr::select(signal, Lower) %>% rename(x=Lower, y=signal) %>% arrange(desc(x))
  )

g1 =
  calibr %>% 
  mutate(conc = ifelse(conc==0, 0.1, conc)) %>% 
  ggplot(aes(x=conc, y=signal)) +
  geom_point(alpha=.5, col="dodgerblue") +
  #geom_segment(aes(x=Lower, xend=Upper, y=signal, yend=signal), data=invpred, col ="green") +
  #geom_polygon(mapping=aes(x=x, y=y), data=invpred_polygon, alpha=.1, fill="green") +
  geom_line(mapping=aes(y=Prediction, x=conc), data=pred, col="dodgerblue") +
  labs(subtitle="Простые координаты",
       x="Концентрация антигена, мкг/мл", 
       y="Сигнал (CPS)")

loglabels = scales::trans_format("log10", scales::math_format(10^.x))
g2 = 
  g1 + 
  scale_x_log10(breaks = 10^(-1:2), labels = loglabels) +
  scale_y_log10(labels = loglabels) +
  coord_cartesian(ylim=c(0.99e3, 3e6), 
                  xlim=c(10^-1.5, 120), 
                  expand = F) + 
  labs(subtitle="logXY координаты") +
  annotation_logticks() 

plot_grid(g1, g2)
```

Отметим, что традиционный способ построения модели основан на наблюдении, что у раститровки в двойных логарифмических координатах есть так называемый "линейный участок", т.е. участок, который в двойных логарифмических координатах выглядит линейным. Разброс (дисперсия) в этих координатах выглядит равномерной (часто), а в прямых координатах он растет с увеличением измеряемых значений сигнала. При этом либо ищется линейная зависимость в двойных логарифмических координатах (степенная функция) $log(\text{signal}) = a + b \cdot log(\text{conc})$, либо подбираются коэффициенты для степенной функции $\text{signal} = e^a \cdot \text{conc}^b = a^* \cdot \text{conc}^b$.

Что с этим способом не так? 

+ Если мы выбрасываем часть измеренных точек, то теряем точность предсказания кривой, ибо выбрасываем часть информации, которая могла бы нами быть использована для повышения точности.
+ Неправильно определяем линейный участок.
+ Не можем определить ошибку предсказания для опытных точек.

```{r cal_lin1, cache=T, echo=F}
# data for plotting
d_cal = 
  calibr %>% 
  mutate(plot_conc = ifelse(conc > 0, 
                            conc, 
                            sort(unique(conc))[2]*.1))

d_cal_subset = 
  d_cal %>% 
  rownames_to_column() %>% 
  mutate(rowname = as.numeric(rowname)) %>% 
  filter(rowname < 17) %>%
  mutate(log_conc = log(plot_conc), # берем логарифмы
         log_signal = log(signal)) 
d_cal_lm = 
  d_cal_subset %>% 
  dplyr::select(log_conc, log_signal) %>%
  filter(log_conc > 0 & log_conc < 3) # double-log-linear range

m_linear = lm(log_signal ~ log_conc, d_cal_lm)

x_linear = seq(min(d_cal$plot_conc),
               max(d_cal$plot_conc),
               l=100) 
p_linear = exp(predict(m_linear, new=data.frame(log_conc=log(x_linear))))


d_linear = data.frame(x=d_cal_subset$plot_conc,
                      y=d_cal_subset$signal,
                      alpha=(log(d_cal_subset$plot_conc) > 0 
                             & log(d_cal_subset$plot_conc) < 3)*.5+.3)

#library(latex2exp)
p_lin_good = 
  d_linear %>% 
  ggplot() + 
  geom_point(aes(x=x, y=y, alpha=alpha), size=2, col="dodgerblue") + 
  scale_x_log10(breaks = 10^(-1:2), labels = loglabels) +
  scale_y_log10(labels = loglabels) +
  coord_cartesian(ylim=c(0.99e3, 3e6), xlim=c(10^-1.5, 120), expand = F) + 
  labs(title="Правильный линейныйучасток",
       subtitle=latex2exp::TeX('$log(signal) = a + b \\cdot log(conc)$'), 
       x='Концентрация, нг/мл', 
       y='Сигнал (например, OD450)') +
  geom_line(data=data.frame(x=x_linear, y=p_linear), aes(x=x, y=y), 
            linetype='dashed', 
            alpha=.7,
            col="dodgerblue") + 
  theme(legend.position = "none") + 
  annotation_logticks() +
  labs(x="Концентрация антигена, мкг/мл", 
       y="Сигнал (CPS)")
```
```{r cal_lin2, cache=T, fig.width=7, fig.height=3.5, echo=F}
d_cal_lm2 = 
  d_cal %>% 
  mutate(log_conc = log(plot_conc), # берем логарифмы
         log_signal = log(signal),
         filt_crit = # double-log-linear range
         floor(log_conc) == 4 | 
         floor(log_conc) == 1 | 
         floor(log_conc) == -1) %>%
    dplyr::select(log_conc, log_signal, filt_crit)

  
m_linear2 = lm(log_signal ~ log_conc, d_cal_lm2 %>% filter(filt_crit))

x_linear2 = seq(min(d_cal$plot_conc),
                max(d_cal$plot_conc),
                l=100) 
p_linear2 = exp(predict(m_linear2, new=data.frame(log_conc=log(x_linear2))))

d_cal_lm2$alpha = ifelse(d_cal_lm2$filt_crit, .7, .2)

p_lin_bad = 
  d_cal_lm2 %>% 
  ggplot() + 
  geom_point(aes(x=exp(log_conc), y=exp(log_signal), alpha=alpha), size=2, col="dodgerblue") + 
  scale_x_log10(breaks = 10^(-1:2), labels = loglabels) +
  scale_y_log10(labels = loglabels) +
  coord_cartesian(ylim=c(0.99e3, 3e6), xlim=c(10^-1.5, 120), expand = F) + 
  geom_line(data=data.frame(x=x_linear2, y=p_linear2), aes(x=x, y=y), 
            linetype='dashed', 
            alpha=.7,
            col="dodgerblue") + 
  theme(legend.position = "none") + 
  annotation_logticks() +
  labs(title="Неправильный линейныйучасток",
       subtitle=latex2exp::TeX('$log(signal) = a + b \\cdot log(conc)$'), 
       x="Концентрация антигена, мкг/мл", 
       y="Сигнал (CPS)")
plot_grid(p_lin_bad, p_lin_good)
```

После того, как калибровка построена (нелинейная модель с 5 параметрами), можно определить по ней концентрацию в пробе с неизвестной концентрацией. 

```{r samples, cache=T, message=F, warning=F}
# данные с https://github.com/lapotok/biochemstat2019/tree/master/data
titrated_sample = rio::import("data/igfbp4_3_titrated_sample.csv")

# сохраняем предсказания
predicted_diluted = 
  ED(m_drc, # модель для рассчета
     respLev = titrated_sample$signal, # для каких значений сигнала предсказываем концентрацию
     interval = "delta", # нужен ли доверительный интервал
     type = "absolute", # задаем абсолютные значения сигнала, а не проценты
     display = F)
# пока у нас предсказания для разбавлений образца
predicted_diluted %<>% as_tibble() 
# пересчитаем то же для неразбавленного образца 
predicted_undiluted = predicted_diluted * titrated_sample$dilution
```

Некоторые точкми выпали из калибровки, некоторые имеют очень большую ошибку. Почему так происходит? Если мы наложим на кривую интервалы предсказаний для всех уровней сигнала для всех разведений образца, то мы увидим, что некоторые образцы слишком концентрированы и находятся на уровне нуля или плато кривой (отсюда большая ошибка), либо даже выходят за эти рамки (отсюда выпадение из калибровки). По величине ошибок предсказаний можно посмотреть, каким точкам стоит доверять. Мы также можем грубо прикинуть по правилу 1.5 IQR, какие из точек являются выбросами (в `R` это можно сделать командой `boxplot(x, plot = F)$out`); на графике красным отмечены выбросы.

```{r range_int, cache=T, fig.width=4, fig.height=3, message=F, warning=F, echo=F}
predicted_undiluted %<>% as_tibble() %>% rownames_to_column() %>% mutate(rowname=as.numeric(rowname))
predicted_undiluted %<>% bind_cols(titrated_sample)

boxplot_outliers = boxplot(predicted_undiluted$Estimate, plot = F)$out
is_outlier = predicted_undiluted$Estimate %in% boxplot_outliers
shape_outlier = ifelse(is_outlier, 4, 19)

g2 + 
  geom_segment(aes(x = Lower/dilution, xend = Upper/dilution, y = signal, yend=signal), 
               data = predicted_undiluted[!is_outlier,], 
               col="green", alpha=.7) +
  geom_segment(aes(x = Lower/dilution, xend = Upper/dilution, y = signal, yend=signal), 
               data = predicted_undiluted[is_outlier,], 
               col="red", alpha=.7) +
  geom_point(aes(x = Estimate/dilution, y = signal), 
               data = predicted_undiluted[!is_outlier,], 
               col="green", alpha=.7) +
  geom_point(aes(x = Estimate/dilution, y = signal), 
               data = predicted_undiluted[is_outlier,], 
               col="red", alpha=.7)
```

Если пересчитать предсказанные концентрации в разбавлениях образца в исходную концентрацию, то мы сами можем увидить выбросы в полученных точках.

```{r box, cache=T, fig.width=4, fig.height=1.5, message=F, warning=F, echo=F}
pb = 
  tibble(y=predicted_undiluted$Estimate) %>% 
  ggplot(aes(x=1, y=y)) +
  geom_boxplot(col="dodgerblue", outlier.shape = NA) + 
  geom_point(data = tibble(y=predicted_undiluted$Estimate[!is_outlier]), 
             position = position_jitter(.05),
             col="dodgerblue",
             alpha=.3,
             size=2) + 
  geom_point(data = tibble(y=predicted_undiluted$Estimate[is_outlier]), 
             col="red",
             alpha=.6,
             size=2) + 
  labs(subtitle = "Оценки и выбросы", y = "Estimate") + 
  coord_flip() + theme_minimal() +
  theme(axis.ticks.y = element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.line.y = element_blank()) 
pb
```

Также мы можем для каждой оценки образца в пересчете на исходную концентрацию посмотреть на ошибки предсказания. Явно, что выбросы надо убрать.

```{r errors, cache=T, fig.width=5, fig.height=4, message=F, warning=F}
pr =
  predicted_undiluted %>% 
  ggplot(aes(y=rowname, x=Estimate, xmin=Lower, xmax=Upper, col=titrated_sample$dilution)) +
  geom_point(size =(titrated_sample$dilution)^(1/8)*1.5,
             shape = shape_outlier) +
  geom_linerangeh() +
  scale_color_gradient(low = "blue", high="red", name="dilution") + 
  scale_y_continuous(breaks = 1:31) +
  theme(axis.ticks.y = element_blank(),
        axis.title.y = element_blank()) + 
  labs(subtitle = "Оценки концентрации и их ошибки")

pr
```

Что же со всем этим делать? После удаления выбросов обычно люди усредняют оценки и выдают полученное число за искомую концентрацию в образце. Однако таким образом мы не сможем указать ошибку этой оценки, а также мы не принимаем в расчет то, что для каждой точки разведения у нас есть своя ошибка. Для того, чтобы сводить воедино несколько оценок с разными ошибками можно использовать мета-анализ.

## Мета-анализ - пример

Мета-анализ обычно используют, когда нужно обобщить результаты из нескольких исследований, от нескольких лабораторий. 

При этом разные исследования могут быть попытками воспроизведения одной и той же методики на одном и том же объекте и получать оценку одной и той же величины. В таком случае, в разных исследованиях мы будем видеть случайные отклонения от единого истинного **фиксированного** значения эффекта (это мета-анализ фиксированных эффектов).

Однако также разные исследования могут пытаться перепроверить некую величину разными методами или на разных объектах (видах животных). Таким образом, оценки получаемые разными исследованиями будут отличаться не только ошибками измерения, но и различиями в объектах, методах - т.е. каждый раз будет измеряться какая-то своя величина со своей ошибкой. И в итоге анализ пытается установить, какую общую величину представляют все эти частные проявления величины, которые все эти исследования по-своему пытаются оценить (это мета-анализ случайных эффектов).

В используемом примере с оценками концентрации по разным точкам образца (одним и тем же методом, одна и та же проба) нас будет интересовать мета-анализ фиксированных эффектов.

```{r meta, cache=T, fig.width=10, fig.height=3.5}
suppressPackageStartupMessages(library(meta))
suppressPackageStartupMessages(library(metafor))

# возьмем первые несколько оценок
ix = 2:8
estimates = predicted_undiluted[ix,]

# запускаем мета-анализ
m_res = metagen(TE = estimates$Estimate, # точечные оценки
                seTE = estimates$`Std. Error`, # ошибки
                studlab = as.character(ix)) # названия для точек

# строим график
m_res %>% forest(xlim=c(50, 200))
```

Видно, что данные точки хорошо согласуются друг с другом (т.к. в них не попали выбросы). Мы видим, что средняя оценка по результатам мета-анализа составило $138.3\ [130.9, 145.72]$.

Однако если включить в анализ точки с выбросами, то они начинают "перетягивать" среднее в свою сторону. В данном случае мета-анализ случайных эффектов оказывается ближе к оценке без выбросов, т.к. он "воспринимает" отклонение выброса, как некую индивидуальную особенность данного "исследования" и не позволяет ей существенно искажать результат.

```{r meta2, cache=T, fig.width=10, fig.height=3.5}
# возьмем первые 8 оценок
ix = 18:24
estimates = predicted_undiluted[ix,]

# запускаем мета-анализ
m_res = metagen(TE = estimates$Estimate, # точечные оценки
                seTE = estimates$`Std. Error`, # ошибки
                studlab = as.character(ix)) # названия для точек

# строим график
m_res %>% forest(xlim=c(0, 450))
```

# Корреляция

Корреляция проверяет наличие совместного возрастания/убывания значений какого-то признака. В отличие от регрессии она симметрична. Для измерения степени взаимосвязи используют коэффициент корреляции. Если взаимодействие линейно, то используют коэффициент линейной корреляции Пирсона, если наблюдается нелинейная монотонная взаимосвязь - ранговые коэффициенты корреляции. Если признаков много - можно расчитывать корреляции для каждой пары.

Значения коэффициентов корреляции находятся в области от -1 до 1. При этом значению 0 соответствует ситуация отсутствия корреляции, а чем больше значение по модулю, тем сильнее взаимодействие. 

Градации коэффициента корреляции (по модулю, корреляция бывает положительная и отрицательная):

+ 0-0.3 - очень слабая;
+ 0.3-0.5 - слабая;
+ 0.5-0.7 - средняя;
+ 0.7-0.9 - высокая;
+ 0.9-1 - очень высокая.

Кроме значения коэффициента очень важно смотреть на его статистическую значимость. Если значение незначимо <отличается от 0> (каким бы оно ни было) - значит мы ничего не можем сказать о силе корреляции. Наконец, всегда полезно смотреть на графики попарных взаимодействий, чтобы быть в курсе особенностей данных, которые не адекватно описываются коэффициентом корреляции, например, нелинейность данных.

Рассмотрим пример инструментов для анализа корреляций в `R`. Загрузим и подготовим данные.

```{r}
# >>> https://rcompanion.org/handbook/I_10.html
# Brendon Small and company recorded several measurements for students in their classes 
# related to their nutrition education program: Grade, Weight in kilograms, intake 
# of Calories per day, daily Sodium intake in milligrams, and Score 
# on the assessment of knowledge gain.

# данные с https://github.com/lapotok/biochemstat2019/tree/master/data
load("data/BrendonSmall_nutrition.RData")

# оставим только численные переменные
BrendonSmall_nutrition_num = BrendonSmall_nutrition %>% select(-Instructor)
BrendonSmall_nutrition_num %>% head()
```

Посчитаем значения одного из коэффициентов корреляции и его уровня значимости.

```{r}
# r and p-values
cor.test(BrendonSmall_nutrition_num[,5], BrendonSmall_nutrition_num[,4])
```

Мы видим, что само значение коэффициента корреляции небольшое, но оно значимо отличается от нуля (см. доверительный интервал или *p-value* теста на равенство коэффициента корреляции нулю).

Можно посмотреть на таблицу всех коэффициентов корреляции, но без значимостей эти коэффициенты на мало о чем говорят!

```{r}
# cor matrix
BrendonSmall_nutrition_num_cor = BrendonSmall_nutrition_num %>% cor(method = "pearson") 
BrendonSmall_nutrition_num_cor %>% round(2)
```

Построим графики попарных взаимодействий. Есть разные пакеты, которые показывают корреляции по-разному.

Построим просто точки. Можно для наглядности покрасить точки в цвета категорийного признака (для которого нельзя посчитать корреляции).

```{r}
pairs(BrendonSmall_nutrition_num, col = BrendonSmall_nutrition$Instructor)
```

Точки, зависимости, коэффициенты и их значимость.

```{r, message=F, warning=F}
PerformanceAnalytics::chart.Correlation(BrendonSmall_nutrition_num, method = "pearson")
# пример с незначимыми коэффицентами
PerformanceAnalytics::chart.Correlation(iris[, 1:4], method = "pearson")
```

Коэффициенты и их значимость.

```{r, fig.width=4, fig.height=4}
iris_cor_p = corrplot::cor.mtest(iris[, 1:4], conf.level = .95)$p
corrplot::corrplot.mixed(cor(iris[, 1:4]), upper="ellipse", p.mat=iris_cor_p)
```